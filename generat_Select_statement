import org.apache.spark.sql.{SparkSession, functions => F}
import org.apache.spark.sql.types._

object DemoCheck extends App {
  // Initialize Spark session
  val spark = SparkSession.builder
    .appName("DemoCheck")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  // Sample data with ArrayType column
  val data = Seq(
    (Array("{\"eye_color\": null}"), "123", "John"),
    (Array("{\"eye_color\": \"blue\"}"), "124", "Jane")
  )

  // Create DataFrame
  val df = data.toDF("eye_color_array", "student_id", "name")

  // Function to cast ArrayType columns containing JSON null to StringType
  def castArrayTypeColumnsToString(df: org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame = {
    val arrayColumnsWithJsonNull = df.columns.filter { columnName =>
      df.schema(columnName).dataType match {
        case ArrayType(StringType, _) =>
          val containsJsonNull = df.select(columnName).as[Array[String]].collect().exists(_.exists(_.contains("\"eye_color\": null")))
          containsJsonNull
        case _ => false
      }
    }

    arrayColumnsWithJsonNull.foldLeft(df) { (tempDf, columnName) =>
      tempDf.withColumn(columnName, F.col(columnName).cast(StringType))
    }
  }

  // Apply the function to cast appropriate columns
  val newDf = castArrayTypeColumnsToString(df)

  // Show the DataFrame
  newDf.show()
  newDf.printSchema()

  // Stop the Spark session
  spark.stop()
}
