import org.apache.spark.sql.{SparkSession, Row, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder.appName("example").getOrCreate()
import spark.implicits._

// Sample data simulating reading from a CSV
val data = Seq(
  ("suszan kapoor", """[{"label":"color","value":"black"}, {"label":"gift","value":"no"}]""")
).toDF("lastname", "demo")

// Define the schema for the JSON data
val jsonSchema = ArrayType(StructType(Seq(
  StructField("label", StringType, true),
  StructField("value", StringType, true)
)))

// Parse the JSON string into an array of structs
val parsedDF = data.withColumn("demo", from_json($"demo", jsonSchema))

// Extract key-value pairs as a sequence of tuples
val keyValueSeqDF = parsedDF.withColumn("demo", expr("transform(demo, x -> (x.label, x.value))"))

// Show the resulting DataFrame
keyValueSeqDF.show(false)

// Collect the data as a sequence of tuples
val result = keyValueSeqDF.select("demo").as[Seq[(String, String)]].collect()

// Print the result
result.foreach(println)
