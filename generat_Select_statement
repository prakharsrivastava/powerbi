import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// Assuming df is your DataFrame with a column named timestampColumn of type Timestamp
val df = // ...

// Define the case class representing your data
case class User(yourColumnName: Option[(java.sql.Timestamp, Long)])

// Define the new schema with a single struct column
val newSchema = StructType(Seq(
  StructField("yourColumnName", DataTypes.createStructType(Array(
    StructField("timestampValue", DataTypes.TimestampType, nullable = true),
    StructField("longValue", DataTypes.LongType, nullable = true)
  )), nullable = true)
))

// Convert the timestamp column to a struct column and cast it to the case class
val updatedDf = df.withColumn("yourColumnName", struct(col("timestampColumn"), lit(null).cast(DataTypes.LongType)))

// Show the updated DataFrame
updatedDf.as[User].show()
