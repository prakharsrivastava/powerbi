import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

object CsvToStructuredDataFrame extends App {
  // Initialize SparkSession
  val spark = SparkSession.builder
    .appName("CSV to Structured DataFrame")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  // Example CSV data
  val csvData = Seq(
    "suszan kapoor,\"{\"color\",\"black\"},{\"gift\",\"no\"}\""
  ).toDF("data")

  // Split CSV data into separate columns
  val data = csvData.selectExpr(
    "split(data, ',\"')[0] as lastname",
    "concat('[', split(data, ',\"')[1], ']') as demo"
  )

  // Define the target schema
  val targetArraySchema = ArrayType(StructType(Seq(
    StructField("label", StringType, true),
    StructField("value", StringType, true)
  )))

  // Define a UDF to convert the CSV-like string to array of structs
  val parseCsvToArray = udf((csv: String) => {
    csv.stripPrefix("[{").stripSuffix("}]").split("},\\s*\\{").map { pair =>
      val Array(key, value) = pair.split("\",\"")
      Row(key.trim.stripPrefix("\"").stripSuffix("\""), value.trim.stripPrefix("\"").stripSuffix("\""))
    }
  }, targetArraySchema)

  // Convert demo column from CSV-like string to Array of Structs
  val dataWithArray = data.withColumn("demo", parseCsvToArray($"demo"))

  // Explode the array of structs into individual rows
  val explodedData = dataWithArray.withColumn("demo", explode($"demo"))

  // Pivot the DataFrame to get a wide format
  val pivotedData = explodedData
    .groupBy("lastname")
    .pivot("demo.label")
    .agg(first("demo.value"))

  // Show the result
  pivotedData.show(false)
}
