import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

object Main {
  def main(args: Array[String]): Unit = {
    // Create SparkSession
    val spark = SparkSession.builder()
      .appName("ScalaMapExample")
      .master("local[2]")
      .getOrCreate()

    // Sample DataFrame
    val df: DataFrame = spark.createDataFrame(Seq(
      ("ginny", "1", "2019-09-15T00:00:00.000Z", "2019-12-15T00:00:00.000Z", "Y", "Y", "N", "N", "N", "N", "male", "White", "MALE", "WHITE", "year2020-2021", "1", "1", "hogwarts", "gryffindor", "English",1),
      ("ginny", "2", "2020-09-15T00:00:00.000Z", "2020-12-15T00:00:00.000Z", "N", "N", "Y", "N", "N", "N", "female", "Asian", "FEMALE", "ASIAN", "year2021-2022", "2", "2", "hogwarts", "gryffindor", "English",1)
    )).toDF("raw_student_id", "Student_Grade", "Completion_Date", "next_completion_date", "Diagnostic_used_to_establish_Growth_Measures_YN", "Economically_Disadvantaged", "English_Language_Learner", "Hispanic_or_Latino", "Migrant", "Special_Education", "Gender", "Race", "GenderEnum", "RaceEnum", "Academic_Year", "Lesson_Grade", "Lesson_Level", "externalAccountId", "school_id", "Diagnostic_Language","report_window")

    // Group DataFrame by "raw_student_id", "report_window", and "Diagnostic_used_to_establish_Growth_Measures_YN"
    val dff = df.groupBy("raw_student_id", "report_window", "Diagnostic_used_to_establish_Growth_Measures_YN").agg(
      count("*").as("count"),
      first("report_window").as("reporting_window"),
      first("Diagnostic_used_to_establish_Growth_Measures_YN").as("Diagnostic_used_to_establish_Growth_Measures_YN")
    )

    // Print DataFrame dff
    println("DataFrame dff:")
    dff.show()

    // Collect DataFrame dff and create map
    val resultMap: Map[String, List[(String, String, String)]] = dff.collect().map { row =>
      val raw_student_id = row.getString(0)
      val report_window = row.getString(1)
      val diagnostic_used = row.getString(2)
      val count = row.getLong(3)
      raw_student_id -> List((count.toString, report_window, diagnostic_used))
    }.toMap

    // Print the created map
    println("Map created from DataFrame:")
    resultMap.foreach(println)

    // Stop SparkSession
    spark.stop()
  }
}
