import org.apache.spark.sql.{SparkSession, Row, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder.appName("example").getOrCreate()
import spark.implicits._

// Sample data simulating reading from a CSV
val data = Seq(
  ("suszan kapoor", """[{"color","black"}, {"gift","no"}]""")
).toDF("lastname", "demo")

// Define a UDF to parse the string and convert it into a sequence of tuples
val parseStringToSeq = udf((str: String) => {
  val regex = "\\{\"(.*?)\",\"(.*?)\"\\}".r
  regex.findAllIn(str).matchData.map { m =>
    (m.group(1), m.group(2))
  }.toSeq
})

// Apply the UDF to the DataFrame
val resultDF = data.withColumn("demo_seq", parseStringToSeq($"demo"))

// Show the resulting DataFrame
resultDF.show(false)

// Collect and print the result
val result = resultDF.select("demo_seq").as[Seq[(String, String)]].collect()
result.foreach(println)
