import org.apache.spark.sql.types._

object ConvertArrayType extends App {
  // Define the original schema
  val originalSchema = StructType(Seq(
    StructField("lastname", StringType),
    StructField("demo", ArrayType(StructType(Seq(
      StructField("label", StringType),
      StructField("value", StringType)
    ))))
  ))

  // Function to convert ArrayType to StringType in the schema without using schema.map
  def convertSchema(schema: StructType): StructType = {
    val newFields = schema.fields.map {
      case StructField(name, arrayType: ArrayType, nullable, metadata) =>
        StructField(name, StringType, nullable, metadata)
      case StructField(name, structType: StructType, nullable, metadata) =>
        StructField(name, convertSchema(structType), nullable, metadata)
      case other => other
    }
    StructType(newFields)
  }

  // Convert the schema
  val newSchema = convertSchema(originalSchema)

  // Print the original and new schema
  println("Original Schema:")
  println(originalSchema.prettyJson)

  println("New Schema:")
  println(newSchema.prettyJson)
}
