import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

object ReadAndParseDemoColumn extends App {
  // Initialize SparkSession
  val spark = SparkSession.builder
    .appName("Read and Parse Demo Column")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  // Example CSV data (assuming data is correctly formatted in CSV)
  val csvData =
    """
      |lastname,demo
      |suzan kapoor,"{""color"":""black"", ""gift"":""no""}"
      |john doe,"{""color"":""blue"", ""gift"":""yes""}"
      |""".stripMargin

  // Define the schema for the DataFrame
  val schema = StructType(Seq(
    StructField("lastname", StringType, true),
    StructField("demo", StringType, true)
  ))

  // Read CSV data with defined schema
  val df = spark.read
    .option("header", "true")
    .schema(schema)
    .csv(spark.sparkContext.parallelize(csvData.split("\n").toSeq).toDS())

  // Function to parse the demo column into StructType
  val parseDemoColumn = udf((demo: String) => {
    val keyValuePairs = demo.stripPrefix("{").stripSuffix("}")
      .split(", ")
      .map { pair =>
        val Array(key, value) = pair.split(":")
        (key.stripPrefix("\"").stripSuffix("\""), value.stripPrefix("\"").stripSuffix("\""))
      }
      .toSeq
    keyValuePairs.map(pair => Row(pair._1, pair._2))
  })

  // Define the target schema for the parsed data
  val targetSchema = StructType(Seq(
    StructField("label", StringType, true),
    StructField("value", StringType, true)
  ))

  // Apply UDF to parse demo column and convert to DataFrame
  val parsedDF = df.withColumn("parsed_demo", explode(parseDemoColumn($"demo")))
    .select($"lastname", struct($"parsed_demo.*").as("demo"))
    .groupBy("lastname")
    .agg(collect_list($"demo").as("demo"))
    .select($"lastname", $"demo".cast(ArrayType(targetSchema)))

  // Show DataFrame
  parsedDF.show(false)

  // Print DataFrame schema
  parsedDF.printSchema()

  // Stop SparkSession
  spark.stop()
}
