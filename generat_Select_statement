import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

object ConvertJsonStringToColumns extends App {
  // Initialize SparkSession
  val spark = SparkSession.builder
    .appName("Convert Json String to Columns")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  // Example DataFrame with StringType column containing JSON-like strings
  val data = Seq(
    ("Smith", """[{"label":"EYEcolor","value":"blue"},{"label":"gifterd","value":null}]"""),
    ("Johnson", """[{"label":"EYEcolor","value":"green"},{"label":"gifterd","value":"yes"}]""")
  ).toDF("lastname", "demo")

  // Define the schema for the JSON string
  val jsonSchema = ArrayType(StructType(Seq(
    StructField("label", StringType, true),
    StructField("value", StringType, true)
  )))

  // Extract the JSON-like strings into new columns
  val extractedDF = data.withColumn("demo_extracted", from_json($"demo", jsonSchema))

  // Explode the array into individual rows
  val explodedDF = extractedDF.withColumn("demo", explode($"demo_extracted"))

  // Pivot the exploded rows to columns
  val pivotedDF = explodedDF
    .select($"lastname", $"demo.label", $"demo.value")
    .groupBy("lastname")
    .pivot("label")
    .agg(first("value"))

  // Show the transformed DataFrame
  pivotedDF.show(false)
  pivotedDF.printSchema()
}
