import org.apache.spark.sql.types._

// Define a function to revert a specific column from StringType to ArrayType
def revertStringToArray(schema: StructType, columnName: String, arrayElementType: DataType): StructType = {
  val newFields = schema.fields.map {
    case StructField(name, StringType, nullable, metadata) if name == columnName =>
      StructField(name, ArrayType(arrayElementType), nullable, metadata)
    case StructField(name, struct: StructType, nullable, metadata) =>
      StructField(name, revertStringToArray(struct, columnName, arrayElementType), nullable, metadata)
    case other =>
      other
  }
  StructType(newFields)
}

// Example usage:
val originalSchema = StructType(Seq(
  StructField("lastname", StringType, nullable = true),
  StructField("demo", StringType, nullable = true),
  StructField("details", StructType(Seq(
    StructField("age", IntegerType, nullable = true),
    StructField("address", StringType, nullable = true)
  )), nullable = true)
))

// Convert "demo" column from StringType to ArrayType of StringType
val updatedSchema = revertStringToArray(originalSchema, "demo", StringType)

// Print the updated schema
println("Updated Schema:")
updatedSchema.printTreeString()
