import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder.appName("example").getOrCreate()
import spark.implicits._

// Sample data simulating reading from a CSV
val data = Seq(
  ("suszan kapoor", """[{"color","black"}, {"gift","no"}]""")
).toDF("lastname", "demo")

// Define a UDF to parse the string and convert it into a sequence of tuples
val parseStringToSeq = udf((str: String) => {
  // Remove leading and trailing brackets
  val cleanedStr = str.stripPrefix("[").stripSuffix("]")
  // Split the string into individual key-value pairs
  val pairs = cleanedStr.split("\\}, \\{")
  pairs.map { pair =>
    // Remove curly braces and split by comma to get key and value
    val keyValue = pair.replaceAll("[\\{\\}\"]", "").split(",")
    (keyValue(0).trim, keyValue(1).trim)
  }.toSeq
})

// Apply the UDF to the DataFrame
val resultDF = data.withColumn("demo_seq", parseStringToSeq($"demo"))

// Show the resulting DataFrame
resultDF.show(false)

// Collect and print the result
val result = resultDF.select("demo_seq").as[Seq[(String, String)]].collect()
result.foreach(println)
