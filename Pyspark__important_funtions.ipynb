{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO9v2TlBmPgUMl3bU4GFqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakharsrivastava/powerbi/blob/main/Pyspark__important_funtions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "IpYZ3cRGEMKi",
        "outputId": "c5d3aada-2dfa-4e57-aae5-adff857374a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/chapter2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/chapter2/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark \n",
        "!pip install  pyspark\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCrzFNAWxB9w",
        "outputId": "0ef22a71-4ba5-4186-f770-91a3d22267cc"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.8/dist-packages (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"architectinaction.com\").getOrCreate()\n",
        "sc= spark.sparkContext"
      ],
      "metadata": {
        "id": "2GRvpurTxEOc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])"
      ],
      "metadata": {
        "id": "LKPbBIrOxIct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rddCollect = rdd.collect()\n",
        "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
        "print(\"Action: First element: \"+str(rdd.first()))\n",
        "print(rddCollect)"
      ],
      "metadata": {
        "id": "04bipVVpxk5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "emptyRDD = sc.emptyRDD()\n",
        "emptyRDD2 = rdd=sc.parallelize([])\n",
        "\n",
        "print(\"is Empty RDD : \"+str(emptyRDD2.isEmpty()))"
      ],
      "metadata": {
        "id": "3qZ5uhzsxr_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create spark session with local[5]\n",
        "rdd = sc.parallelize(range(0,20))\n",
        "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
        "\n",
        "# Use parallelize with 6 partitions\n",
        "rdd1 = sc.parallelize(range(0,25), 6)\n",
        "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
        "\n",
        "rddFromFile = sc.textFile(\"readme.html\",10)\n",
        "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
      ],
      "metadata": {
        "id": "OvJDACuSx10c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rdd1.saveAsTextFile(\"partition\")\n"
      ],
      "metadata": {
        "id": "VF-00855zSC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using repartition\n",
        "rdd2 = rdd1.repartition(4)\n",
        "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
        "rdd2.saveAsTextFile(\"re-partition\")"
      ],
      "metadata": {
        "id": "Ra_nVqXpzoUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using coalesce()\n",
        "rdd3 = rdd1.coalesce(4)\n",
        "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
        "rdd3.saveAsTextFile(\"coalesce\")"
      ],
      "metadata": {
        "id": "T3qMaqCUz2x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType\n",
        "from pyspark.sql.functions import col,struct,when\n",
        "\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "  ]\n",
        "\n",
        "spark.createDataFrame(data).show(truncate=False)\n",
        "schema = StructType([ \n",
        "    StructField(\"firstname\",StringType(),True), \n",
        "    StructField(\"middlename\",StringType(),True), \n",
        "    StructField(\"lastname\",StringType(),True), \n",
        "    StructField(\"id\", StringType(), True), \n",
        "    StructField(\"gender\", StringType(), True), \n",
        "    StructField(\"salary\", IntegerType(), True) \n",
        "  ])\n",
        " \n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "UNmSP8fD1p5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "structureData = [\n",
        "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
        "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
        "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
        "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
        "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
        "  ]\n",
        "\n",
        "spark.createDataFrame(structureData).show(truncate=False)\n",
        "structureSchema = StructType([\n",
        "        StructField('name', StructType([\n",
        "             StructField('firstname', StringType(), True),\n",
        "             StructField('middlename', StringType(), True),\n",
        "             StructField('lastname', StringType(), True)\n",
        "             ])),\n",
        "         StructField('id', StringType(), True),\n",
        "         StructField('gender', StringType(), True),\n",
        "         StructField('salary', IntegerType(), True)\n",
        "         ])\n",
        "\n",
        "df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "\n",
        "updatedDF = df2.withColumn(\"OtherInfo\", \n",
        "    struct(col(\"id\").alias(\"identifier\"),\n",
        "    col(\"gender\").alias(\"gender\"),\n",
        "    col(\"salary\").alias(\"salary\"),\n",
        "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
        "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
        "      .otherwise(\"High\").alias(\"Salary_Grade\")\n",
        "  ))\n",
        "#.drop(\"id\",\"gender\",\"salary\")\n",
        "\n",
        "updatedDF.printSchema()\n",
        "updatedDF.show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "nKOd-eeM1zRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster "
      ],
      "metadata": {
        "id": "reohFs1G2f6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = sc.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "df=spark.createDataFrame(rdd)\n",
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]\n",
        "\n",
        "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
        "print(result)"
      ],
      "metadata": {
        "id": "OeZiYzvpk0j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "6t2iNDFQlkks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filteDf= rdd.where((rdd['state'].isin(broadcastStates.value)))\n"
      ],
      "metadata": {
        "id": "1WTBwG98lMDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df=df.withColumn(\"name\",lit(\"architectInAction\"))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "wV-dFyWMnSYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.reshape.api import concat\n",
        "df=df.withColumn(\"name2\",lit(\"name\"))\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VkHlFbZooCcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat,col\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df2=df.withColumn(\"name3\",concat(col(\"_1\"),lit(\"777\")))\n",
        "             \n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "rrrSqkdxqQd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"name\").show()\n",
        "df.select(df.name).show()\n",
        "df.select(df[\"name\"]).show()\n",
        "\n",
        "#By using col() function\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col(\"name\")).show()\n",
        "\n",
        "#Select columns by regular expression\n",
        "df.select(df.colRegex(\"`^.*name*`\")).show()"
      ],
      "metadata": {
        "id": "MhWs126PnqOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.distinct().show()\n",
        "df.select(\"_6\").distinct().show()\n",
        "df.dropDuplicates().show()"
      ],
      "metadata": {
        "id": "K4d-0FwP2jxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.distinct().show()\n",
        "print(df.select(\"_6\").distinct().count())\n",
        "df.dropDuplicates().show()"
      ],
      "metadata": {
        "id": "ePjUmIsaFQfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.select(countDistinct(\"_2\", \"_6\"))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "0EMVF8HkF3xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#group by\n",
        "#agg() – Using groupBy() agg() function, we can calculate more than one aggregate at a time."
      ],
      "metadata": {
        "id": "h0uVA2T23NPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"_5\").sum(\"_6\").show(truncate=False)\n"
      ],
      "metadata": {
        "id": "C2BZcup33UAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"_5\").min(\"_6\").show()"
      ],
      "metadata": {
        "id": "qiNW9AwU3iXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"_5\") .agg(sum(\"_6\").alias(\"sum_salary\"),avg(\"_6\").alias(\"avg_salary\") ).show(truncate=False)"
      ],
      "metadata": {
        "id": "t-NXf7bk31L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"_5\") .agg(sum(\"_6\").alias(\"sum_salary\"),avg(\"_6\").alias(\"avg_salary\") ).where(col(\"avg_salary\") >= 3000).show(truncate=False)"
      ],
      "metadata": {
        "id": "_6rYfFsC4G_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.join(df,df._5 ==  df._5,\"inner\") \\\n",
        "     .show(truncate=False)"
      ],
      "metadata": {
        "id": "H0VwZU1R4bSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.union(df)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "I95QUkoR4m1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "upperCaseUDF = udf(lambda z:upperCase(z),StringType())    \n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
        ".show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xh2XJeOF4yku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.pandas as ps\n",
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame\n",
        "psdf = ps.DataFrame(df)\n",
        "psdf\n"
      ],
      "metadata": {
        "id": "Gb9mGEZL8Sfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add(data):\n",
        "   return data[0] + \"ffff\"\n",
        "   \n",
        "psdf = psdf.apply(add,axis=1)\n",
        "psdf.head()"
      ],
      "metadata": {
        "id": "4IXayKaB8d-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = ((\"Java\",4000,5), \\\n",
        "    (\"Python\", 4600,10),  \\\n",
        "    (\"Scala\", 4100,15),   \\\n",
        "    (\"Scala\", 4500,15),   \\\n",
        "    (\"PHP\", 3000,20),  \\\n",
        "  )\n",
        "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Custom transformation 1\n",
        "from pyspark.sql.functions import upper\n",
        "def to_upper_str_columns(df):\n",
        "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
        "\n",
        "# Custom transformation 2\n",
        "def reduce_price(df,reduceBy):\n",
        "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
        "\n",
        "# Custom transformation 3\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\",  \\\n",
        "             df.new_fee - (df.new_fee * df.discount) / 100)\n",
        "\n",
        "# transform() usage\n",
        "df2 = df.transform(to_upper_str_columns) \\\n",
        "        .transform(reduce_price,1000) \\\n",
        "        .transform(apply_discount) \n",
        "                \n",
        "df2.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDukU_k5BLas",
        "outputId": "039b5bc0-a9da-4348-9293-f43c8729c7af"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CourseName: string (nullable = true)\n",
            " |-- fee: long (nullable = true)\n",
            " |-- discount: long (nullable = true)\n",
            "\n",
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "|Scala     |4500|15      |\n",
            "|PHP       |3000|20      |\n",
            "+----------+----+--------+\n",
            "\n",
            "+----------+----+--------+-------+--------------+\n",
            "|CourseName| fee|discount|new_fee|discounted_fee|\n",
            "+----------+----+--------+-------+--------------+\n",
            "|      JAVA|4000|       5|   3000|        2850.0|\n",
            "|    PYTHON|4600|      10|   3600|        3240.0|\n",
            "|     SCALA|4100|      15|   3100|        2635.0|\n",
            "|     SCALA|4500|      15|   3500|        2975.0|\n",
            "|       PHP|3000|      20|   2000|        1600.0|\n",
            "+----------+----+--------+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = [\"Project Gutenberg’s\",\n",
        "        \"Alice’s Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\",\n",
        "        \"Adventures in Wonderland\",\n",
        "        \"Project Gutenberg’s\"]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "rdd.collect()\n"
      ],
      "metadata": {
        "id": "wNFjFEXWB4eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
        "rdd2.collect()\n",
        " "
      ],
      "metadata": {
        "id": "8xvVF_P1CAPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "id": "7pJpxR2SDZdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#overwrite\n",
        "df2.write.mode('append').option(\"header\",True) \\\n",
        "        .option(\"maxRecordsPerFile\", 5) \\\n",
        "        .partitionBy(\"fee\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .csv(\"course\")"
      ],
      "metadata": {
        "id": "5V2DkeebDIqh"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqUV25UCTpq_",
        "outputId": "8c658f15-9707-41d9-816c-76459a7d9605"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[CourseName: string, fee: bigint, discount: bigint, new_fee: bigint, discounted_fee: double]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
        "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
        "        (\"Jen\",\"\",None)]\n",
        "columns = [\"name\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "UiueTLRyH1Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
        "                                 .when(df.gender == \"F\",\"Female\")\n",
        "                                 .when(df.gender.isNull() ,\"\")\n",
        "                                 .otherwise(df.gender))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "rNk2gWJhJAbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, col\n",
        "\n",
        "#Using Case When on withColumn()\n",
        "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
        "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
        "               \"ELSE gender END\"))\n",
        "df3.show(truncate=False)"
      ],
      "metadata": {
        "id": "yd5nc_K-JcrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.createOrReplaceTempView(\"EMP\")\n",
        "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
        "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
        "              \"ELSE gender END as new_gender from EMP\").show()"
      ],
      "metadata": {
        "id": "xQKHWAGsJjca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
        "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.write.mode(\"overwrite\").parquet(\"people.parquet\")\n",
        "parDF1=spark.read.parquet(\"people.parquet\")\n",
        "parDF1.createOrReplaceTempView(\"parquetTable\")\n",
        "parDF1.printSchema()\n",
        "parDF1.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HJOnYAGKGwr",
        "outputId": "d9d4f4cb-5740-4b8d-a830-b71a1ab58273"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|dob  |gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|James    |          |Smith   |36636|M     |3000  |\n",
            "|Michael  |Rose      |        |40288|M     |4000  |\n",
            "|Robert   |          |Williams|42114|M     |4000  |\n",
            "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
            "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
        "parkSQL.show(truncate=False)\n",
        "\n",
        "\n",
        "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"people.parquet\")\n",
        "\n",
        "parDF2=spark.read.parquet(\"people.parquet/gender=M\")\n",
        "parDF2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22mpodbrVnEU",
        "outputId": "c70004eb-4750-402d-be64-22e4520e8991"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|dob  |gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|Michael  |Rose      |        |40288|M     |4000  |\n",
            "|Robert   |          |Williams|42114|M     |4000  |\n",
            "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n",
            "+---------+----------+--------+-----+------+\n",
            "|firstname|middlename|lastname|dob  |salary|\n",
            "+---------+----------+--------+-----+------+\n",
            "|James    |          |Smith   |36636|3000  |\n",
            "|Michael  |Rose      |        |40288|4000  |\n",
            "|Robert   |          |Williams|42114|4000  |\n",
            "+---------+----------+--------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"flights_sample.json\")\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIb6ougxWgJW",
        "outputId": "7ac72569-5815-4703-f737-2ce2ae5e2d2d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ARR_DELAY: long (nullable = true)\n",
            " |-- ARR_TIME: string (nullable = true)\n",
            " |-- CANCELLED: boolean (nullable = true)\n",
            " |-- CRS_ARR_TIME: string (nullable = true)\n",
            " |-- CRS_DEP_TIME: string (nullable = true)\n",
            " |-- DEP_DELAY: long (nullable = true)\n",
            " |-- DEP_TIME: string (nullable = true)\n",
            " |-- DEST: string (nullable = true)\n",
            " |-- DEST_AIRPORT_SEQ_ID: string (nullable = true)\n",
            " |-- DISTANCE: string (nullable = true)\n",
            " |-- DIVERTED: boolean (nullable = true)\n",
            " |-- FL_DATE: string (nullable = true)\n",
            " |-- ORIGIN: string (nullable = true)\n",
            " |-- ORIGIN_AIRPORT_SEQ_ID: string (nullable = true)\n",
            " |-- TAXI_IN: long (nullable = true)\n",
            " |-- TAXI_OUT: long (nullable = true)\n",
            " |-- UNIQUE_CARRIER: string (nullable = true)\n",
            " |-- WHEELS_OFF: string (nullable = true)\n",
            " |-- WHEELS_ON: string (nullable = true)\n",
            "\n",
            "+---------+--------+---------+------------+------------+---------+--------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+----------+---------+\n",
            "|ARR_DELAY|ARR_TIME|CANCELLED|CRS_ARR_TIME|CRS_DEP_TIME|DEP_DELAY|DEP_TIME|DEST|DEST_AIRPORT_SEQ_ID|DISTANCE|DIVERTED|   FL_DATE|ORIGIN|ORIGIN_AIRPORT_SEQ_ID|TAXI_IN|TAXI_OUT|UNIQUE_CARRIER|WHEELS_OFF|WHEELS_ON|\n",
            "+---------+--------+---------+------------+------------+---------+--------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+----------+---------+\n",
            "|      -18|    1751|    false|        1809|        1600|       -5|    1555| ATL|            1000101|  692.00|   false|2015-04-28|   ABE|              1000101|      4|       7|            EV|      1602|     1747|\n",
            "|      -19|    0759|    false|        0818|        0600|       -4|    0556| ATL|            1000101|  692.00|   false|2015-11-05|   ABE|              1000101|     10|      12|            DL|      0608|     0749|\n",
            "+---------+--------+---------+------------+------------+---------+--------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "columns = [\"id\", \"name\",\"age\",\"gender\"]\n",
        "\n",
        "# Create DataFrame \n",
        "data = [(1, \"James\",30,\"M\"), (2, \"Ann\",40,\"F\"),\n",
        "    (3, \"Jeff\",41,\"M\"),(4, \"Jennifer\",20,\"F\")]\n",
        "sampleDF = spark.sparkContext.parallelize(data).toDF(columns)\n",
        "\n",
        "# Create Hive Internal table\n",
        "sampleDF.write.mode('overwrite') \\\n",
        "         .saveAsTable(\"employee\")\n",
        "\n",
        "# Read Hive table\n",
        "df = spark.read.table(\"employee\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqzzX0FAXDy0",
        "outputId": "f33176ab-d6c4-42a0-ed74-5e7e5cf634e6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+------+\n",
            "| id|    name|age|gender|\n",
            "+---+--------+---+------+\n",
            "|  1|   James| 30|     M|\n",
            "|  2|     Ann| 40|     F|\n",
            "|  3|    Jeff| 41|     M|\n",
            "|  4|Jennifer| 20|     F|\n",
            "+---+--------+---+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}