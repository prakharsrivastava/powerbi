{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+M2P+5P1VVxwygFC5zvsU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakharsrivastava/powerbi/blob/main/Natural_Language_Processing_with_Python_ch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho88jnDodzsV",
        "outputId": "2c739a7b-6459-4677-d80a-65465c6c21d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azt9d4kZeV92",
        "outputId": "8e97d309-69bc-45b3-9297-05cb2fd7d008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "emma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCjSL4zFfaxL",
        "outputId": "bd9bfeab-b99f-4ae6-dc00-f902158fb5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma.concordance(\"surprize\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmdlRbhniT7R",
        "outputId": "df106f19-b938-463a-cdc5-0a217d998948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 25 of 37 matches:\n",
            "er father , was sometimes taken by surprize at his being still able to pity ` \n",
            "hem do the other any good .\" \" You surprize me ! Emma must do Harriet good : a\n",
            "Knightley actually looked red with surprize and displeasure , as he stood up ,\n",
            "r . Elton , and found to his great surprize , that Mr . Elton was actually on \n",
            "d aid .\" Emma saw Mrs . Weston ' s surprize , and felt that it must be great ,\n",
            "father was quite taken up with the surprize of so sudden a journey , and his f\n",
            "y , in all the favouring warmth of surprize and conjecture . She was , moreove\n",
            "he appeared , to have her share of surprize , introduction , and pleasure . Th\n",
            "ir plans ; and it was an agreeable surprize to her , therefore , to perceive t\n",
            "talking aunt had taken me quite by surprize , it must have been the death of m\n",
            "f all the dialogue which ensued of surprize , and inquiry , and congratulation\n",
            " the present . They might chuse to surprize her .\" Mrs . Cole had many to agre\n",
            "the mode of it , the mystery , the surprize , is more like a young woman ' s s\n",
            " to her song took her agreeably by surprize -- a second , slightly but correct\n",
            "\" \" Oh ! no -- there is nothing to surprize one at all .-- A pretty fortune ; \n",
            "t to be considered . Emma ' s only surprize was that Jane Fairfax should accep\n",
            "of your admiration may take you by surprize some day or other .\" Mr . Knightle\n",
            "ation for her will ever take me by surprize .-- I never had a thought of her i\n",
            " expected by the best judges , for surprize -- but there was great joy . Mr . \n",
            " sound of at first , without great surprize . \" So unreasonably early !\" she w\n",
            "d Frank Churchill , with a look of surprize and displeasure .-- \" That is easy\n",
            "; and Emma could imagine with what surprize and mortification she must be retu\n",
            "tled that Jane should go . Quite a surprize to me ! I had not the least idea !\n",
            " . It is impossible to express our surprize . He came to speak to his father o\n",
            "g engaged !\" Emma even jumped with surprize ;-- and , horror - struck , exclai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fileid in nltk.corpus.gutenberg.fileids():\n",
        " num_chars = len(nltk.corpus.gutenberg.raw(fileid))\n",
        " num_words = len(nltk.corpus.gutenberg.words(fileid))\n",
        " num_sents = len(nltk.corpus.gutenberg.sents(fileid))\n",
        " num_vocab = len(set([w.lower() for w in nltk.corpus.gutenberg.words(fileid)]))\n",
        " print (int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab),fileid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX6O73gcjCA_",
        "outputId": "f8627bde-3060-47be-a72f-79cc9b2e2c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 24 26 austen-emma.txt\n",
            "4 26 16 austen-persuasion.txt\n",
            "4 28 22 austen-sense.txt\n",
            "4 33 79 bible-kjv.txt\n",
            "4 19 5 blake-poems.txt\n",
            "4 19 14 bryant-stories.txt\n",
            "4 17 12 burgess-busterbrown.txt\n",
            "4 20 12 carroll-alice.txt\n",
            "4 20 11 chesterton-ball.txt\n",
            "4 22 11 chesterton-brown.txt\n",
            "4 18 10 chesterton-thursday.txt\n",
            "4 20 24 edgeworth-parents.txt\n",
            "4 25 15 melville-moby_dick.txt\n",
            "4 52 10 milton-paradise.txt\n",
            "4 11 8 shakespeare-caesar.txt\n",
            "4 12 7 shakespeare-hamlet.txt\n",
            "4 12 6 shakespeare-macbeth.txt\n",
            "4 36 12 whitman-leaves.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQTEtmv0l8te",
        "outputId": "d5ec85ef-e694-4ca7-a6e7-75f9c1100423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "macbeth_sentences = gutenberg.words('shakespeare-macbeth.txt')\n",
        "macbeth_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI--h3-amPEj",
        "outputId": "f87df159-493c-4028-caec-f43804d093b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import webtext\n",
        "for fileid in webtext.fileids():\n",
        "    print (fileid, webtext.raw(fileid)[:65])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0GvbBOavmp21",
        "outputId": "8503f20e-e0fd-4fe9-ba53-1a4e473429b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0f3dd4f370f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwebtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'PlaintextCorpusReader' object has no attribute 'posts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import nps_chat\n",
        "for fileid in nps_chat.fileids():\n",
        "    print (fileid, nps_chat.posts (fileid)[123])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtV7bHM2obWm",
        "outputId": "08d557d3-9e1a-41c2-cd10-9d045a8568d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-19-20s_706posts.xml boo\n",
            "10-19-30s_705posts.xml many\n",
            "10-19-40s_686posts.xml U28\n",
            "10-19-adults_706posts.xml ....\n",
            "10-24-40s_706posts.xml PART\n",
            "10-26-teens_706posts.xml if\n",
            "11-06-adults_706posts.xml Lion\n",
            "11-08-20s_705posts.xml ya\n",
            "11-08-40s_706posts.xml U7\n",
            "11-08-adults_705posts.xml ?\n",
            "11-08-teens_706posts.xml up\n",
            "11-09-20s_706posts.xml tips\n",
            "11-09-40s_706posts.xml sowwy\n",
            "11-09-adults_706posts.xml me\n",
            "11-09-teens_706posts.xml love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import nps_chat\n",
        "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
        "chatroom[123]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm_Bsx7koPjt",
        "outputId": "e58e24f1-64c8-452b-da56-0996496cd3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'want',\n",
              " 'hot',\n",
              " 'pics',\n",
              " 'of',\n",
              " 'a',\n",
              " 'female',\n",
              " ',',\n",
              " 'I',\n",
              " 'can',\n",
              " 'look',\n",
              " 'in',\n",
              " 'a',\n",
              " 'mirror',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "SPjaXjdU-8zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eRHjtMX--zB",
        "outputId": "b44033ca-3784-42a4-e8fd-db97542f91ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_text = brown.words(categories='news')\n",
        "fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "for m in modals:\n",
        "  print (m + ':', fdist[m])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9BXe2aD_I8p",
        "outputId": "3eea49bc-2d2b-4f31-8a4b-eb5ba7e7da26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "can: 94\n",
            "could: 87\n",
            "may: 93\n",
            "might: 38\n",
            "must: 53\n",
            "will: 389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfd = nltk.ConditionalFreqDist((genre, word)\n",
        "for genre in brown.categories()\n",
        "  for word in brown.words(categories=genre))\n",
        "\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "cfd.tabulate(conditions=genres, samples=modals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgVprQcP_ZeT",
        "outputId": "999a9b28-303c-46e4-b739-e05c814a7518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  can could   may might  must  will \n",
            "           news    93    86    66    38    50   389 \n",
            "       religion    82    59    78    12    54    71 \n",
            "        hobbies   268    58   131    22    83   264 \n",
            "science_fiction    16    49     4    12     8    16 \n",
            "        romance    74   193    11    51    45    43 \n",
            "          humor    16    30     8     8     9    13 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "puzzle_letters = nltk.FreqDist('egivrvonl')\n",
        "obligatory = 'r'\n",
        "wordlist = nltk.corpus.words.words()\n",
        "[w for w in wordlist if len(w) >= 6\n",
        "and obligatory in w\n",
        "and nltk.FreqDist(w) <= puzzle_letters]"
      ],
      "metadata": {
        "id": "97zXMULBBm6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a9a1b2-8ac4-447e-c5ab-60dcf3225c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glover',\n",
              " 'gorlin',\n",
              " 'govern',\n",
              " 'grovel',\n",
              " 'ignore',\n",
              " 'involver',\n",
              " 'lienor',\n",
              " 'linger',\n",
              " 'longer',\n",
              " 'lovering',\n",
              " 'noiler',\n",
              " 'overling',\n",
              " 'region',\n",
              " 'renvoi',\n",
              " 'revolving',\n",
              " 'ringle',\n",
              " 'roving',\n",
              " 'violer',\n",
              " 'virole']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entries = nltk.corpus.cmudict.entries()\n",
        "len(entries)\n",
        "\n",
        "for entry in entries[39943:39951]:\n",
        "  print(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXVQFLy8DdhY",
        "outputId": "e9d55ea8-63dc-4722-e051-367d0a6a979e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('explorer', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'ER0'])\n",
            "('explorers', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'ER0', 'Z'])\n",
            "('explores', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'Z'])\n",
            "('exploring', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'IH0', 'NG'])\n",
            "('explosion', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'ZH', 'AH0', 'N'])\n",
            "('explosions', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'ZH', 'AH0', 'N', 'Z'])\n",
            "('explosive', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'S', 'IH0', 'V'])\n",
            "('explosively', ['EH2', 'K', 'S', 'P', 'L', 'OW1', 'S', 'IH0', 'V', 'L', 'IY0'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# entries = nltk.corpus.cmudict.entries()          to count distinct, use dict() rather than entries()\n",
        "prondict = nltk.corpus.cmudict.dict()\n"
      ],
      "metadata": {
        "id": "puJcmJOTEjzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# count words in the dictionary that have more than one possible pronunciation\n",
        "# iterate over the dict and find those whose values' length is greater than 1\n",
        "wordPron = 0\n",
        "for key in prondict:\n",
        "    if len(prondict[key]) > 1:\n",
        "        \n",
        "        wordPron += 1\n",
        "print('Fractions of words with more than one possible pronunciation:', wordPron / len(prondict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUOf-sZKD7e2",
        "outputId": "df56e49f-422f-4fa3-fa7d-df7f75325ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fractions of words with more than one possible pronunciation: 0.07485318537118789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syllable = ['N', 'IH0', 'K', 'S']\n",
        "[word for word, pron in entries if pron[-4:] == syllable]"
      ],
      "metadata": {
        "id": "BHSYal9_FdMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(w,pron) for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']"
      ],
      "metadata": {
        "id": "ZAUZKEUwFzmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1632XN7_GK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(set(w for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))"
      ],
      "metadata": {
        "id": "AlaQYI1ZGGLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stress(pron):\n",
        "  return [char for phone in pron for char in phone if char.isdigit()]"
      ],
      "metadata": {
        "id": "rZ1NEohHGik-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(w,pron) for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']]"
      ],
      "metadata": {
        "id": "q1PsARZYGrui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(word, pron) for (word, pron) in entries if pron[0] == 'P' and len(pron) == 3]"
      ],
      "metadata": {
        "id": "jrnnZnaBISFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p3 = [(pron[0]+'-'+pron[2], word)\n",
        "for (word, pron) in entries\n",
        "if pron[0] == 'P' and len(pron) == 3]\n",
        "p3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KEnCCROIPvi",
        "outputId": "9a548c25-d762-414e-ec1b-231ff6726191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('P-P', 'paap'),\n",
              " ('P-P', 'paape'),\n",
              " ('P-R', 'paar'),\n",
              " ('P-SH', 'paasch'),\n",
              " ('P-K', 'pac'),\n",
              " ('P-S', 'pace'),\n",
              " ('P-K', 'pack'),\n",
              " ('P-D', 'pad'),\n",
              " ('P-K', 'paek'),\n",
              " ('P-TH', 'paeth'),\n",
              " ('P-F', 'paff'),\n",
              " ('P-JH', 'page'),\n",
              " ('P-L', 'pahl'),\n",
              " ('P-IY0', 'pai'),\n",
              " ('P-D', 'paid'),\n",
              " ('P-JH', 'paige'),\n",
              " ('P-N', 'paign'),\n",
              " ('P-K', 'paik'),\n",
              " ('P-L', 'pail'),\n",
              " ('P-L', 'paille'),\n",
              " ('P-N', 'pain'),\n",
              " ('P-N', 'paine'),\n",
              " ('P-R', 'pair'),\n",
              " ('P-Z', 'pais'),\n",
              " ('P-T', 'pait'),\n",
              " ('P-Z', 'paiz'),\n",
              " ('P-K', 'pak'),\n",
              " ('P-K', 'pake'),\n",
              " ('P-L', 'pal'),\n",
              " ('P-L', 'pale'),\n",
              " ('P-L', 'pall'),\n",
              " ('P-L', 'pall'),\n",
              " ('P-M', 'palm'),\n",
              " ('P-M', 'palme'),\n",
              " ('P-M', 'pam'),\n",
              " ('P-N', 'pan'),\n",
              " ('P-N', 'pane'),\n",
              " ('P-NG', 'pang'),\n",
              " ('P-Z', \"pao's\"),\n",
              " ('P-P', 'pap'),\n",
              " ('P-P', 'pape'),\n",
              " ('P-P', 'papp'),\n",
              " ('P-K', 'paque'),\n",
              " ('P-R', 'par'),\n",
              " ('P-R', 'pare'),\n",
              " ('P-R', 'parr'),\n",
              " ('P-Z', 'pas'),\n",
              " ('P-SH', 'pash'),\n",
              " ('P-S', 'pass'),\n",
              " ('P-S', 'pasts'),\n",
              " ('P-T', 'pat'),\n",
              " ('P-CH', 'patch'),\n",
              " ('P-T', 'pate'),\n",
              " ('P-TH', 'path'),\n",
              " ('P-TH', 'pathe'),\n",
              " ('P-T', 'patt'),\n",
              " ('P-L', 'paul'),\n",
              " ('P-L', 'paule'),\n",
              " ('P-L', 'paull'),\n",
              " ('P-P', 'paup'),\n",
              " ('P-Z', 'pause'),\n",
              " ('P-CH', 'pautsch'),\n",
              " ('P-V', 'pave'),\n",
              " ('P-D', 'pawed'),\n",
              " ('P-N', 'pawn'),\n",
              " ('P-Z', 'paws'),\n",
              " ('P-ER0', 'payer'),\n",
              " ('P-ER1', 'payeur'),\n",
              " ('P-N', 'payne'),\n",
              " ('P-Z', 'pays'),\n",
              " ('P-Z', 'paz'),\n",
              " ('P-S', 'peace'),\n",
              " ('P-CH', 'peach'),\n",
              " ('P-K', 'peak'),\n",
              " ('P-K', 'peake'),\n",
              " ('P-L', 'peal'),\n",
              " ('P-L', 'peale'),\n",
              " ('P-R', 'pear'),\n",
              " ('P-L', 'pearl'),\n",
              " ('P-L', 'pearle'),\n",
              " ('P-S', 'pearse'),\n",
              " ('P-T', 'peart'),\n",
              " ('P-Z', 'peas'),\n",
              " ('P-Z', 'pease'),\n",
              " ('P-S', 'pease'),\n",
              " ('P-T', 'peat'),\n",
              " ('P-K', 'pech'),\n",
              " ('P-K', 'peck'),\n",
              " ('P-D', 'peed'),\n",
              " ('P-K', 'peek'),\n",
              " ('P-L', 'peel'),\n",
              " ('P-L', 'peele'),\n",
              " ('P-P', 'peep'),\n",
              " ('P-R', 'peer'),\n",
              " ('P-T', 'peet'),\n",
              " ('P-T', 'peete'),\n",
              " ('P-V', 'peeve'),\n",
              " ('P-G', 'peg'),\n",
              " ('P-G', 'pegg'),\n",
              " ('P-L', 'pehl'),\n",
              " ('P-L', 'peil'),\n",
              " ('P-N', 'peine'),\n",
              " ('P-Z', \"pei's\"),\n",
              " ('P-L', 'pell'),\n",
              " ('P-L', 'pelle'),\n",
              " ('P-N', 'pen'),\n",
              " ('P-NG', 'peng'),\n",
              " ('P-N', 'penh'),\n",
              " ('P-N', 'penn'),\n",
              " ('P-P', 'pep'),\n",
              " ('P-AH0', 'pera'),\n",
              " ('P-K', 'perc'),\n",
              " ('P-S', 'perce'),\n",
              " ('P-CH', 'perch'),\n",
              " ('P-K', 'perk'),\n",
              " ('P-L', 'perl'),\n",
              " ('P-L', 'perle'),\n",
              " ('P-OW0', 'pero'),\n",
              " ('P-OW1', 'perot'),\n",
              " ('P-S', 'pers'),\n",
              " ('P-S', 'perse'),\n",
              " ('P-T', 'pert'),\n",
              " ('P-TH', 'perth'),\n",
              " ('P-UW1', 'peru'),\n",
              " ('P-Z', 'perz'),\n",
              " ('P-S', 'pesce'),\n",
              " ('P-SH', 'pesch'),\n",
              " ('P-SH', 'pesh'),\n",
              " ('P-T', 'pet'),\n",
              " ('P-T', 'pete'),\n",
              " ('P-TH', 'peth'),\n",
              " ('P-CH', 'petsch'),\n",
              " ('P-CH', 'petsche'),\n",
              " ('P-T', 'pett'),\n",
              " ('P-UW1', 'peugh'),\n",
              " ('P-UW1', 'pew'),\n",
              " ('P-Z', 'pez'),\n",
              " ('P-AH0', 'pia'),\n",
              " ('P-K', 'pic'),\n",
              " ('P-CH', 'piche'),\n",
              " ('P-K', 'pick'),\n",
              " ('P-S', 'piece'),\n",
              " ('P-CH', 'piech'),\n",
              " ('P-D', 'pied'),\n",
              " ('P-L', 'piehl'),\n",
              " ('P-L', 'piel'),\n",
              " ('P-R', 'pier'),\n",
              " ('P-Z', 'pies'),\n",
              " ('P-T', 'piet'),\n",
              " ('P-CH', 'pietsch'),\n",
              " ('P-T', 'piette'),\n",
              " ('P-G', 'pig'),\n",
              " ('P-G', 'pigg'),\n",
              " ('P-G', 'pigue'),\n",
              " ('P-L', 'pihl'),\n",
              " ('P-K', 'pik'),\n",
              " ('P-K', 'pike'),\n",
              " ('P-L', 'pil'),\n",
              " ('P-L', 'pile'),\n",
              " ('P-L', 'pill'),\n",
              " ('P-L', 'pille'),\n",
              " ('P-M', 'pimm'),\n",
              " ('P-N', 'pin'),\n",
              " ('P-N', 'pine'),\n",
              " ('P-NG', 'ping'),\n",
              " ('P-N', 'pinn'),\n",
              " ('P-OW0', 'pio'),\n",
              " ('P-P', 'pip'),\n",
              " ('P-P', 'pipe'),\n",
              " ('P-P', 'pipp'),\n",
              " ('P-K', 'pique'),\n",
              " ('P-S', 'piss'),\n",
              " ('P-T', 'pit'),\n",
              " ('P-CH', 'pitch'),\n",
              " ('P-TH', 'pith'),\n",
              " ('P-CH', 'pitsch'),\n",
              " ('P-T', 'pitt'),\n",
              " ('P-AA1', 'pla'),\n",
              " ('P-EY1', 'play'),\n",
              " ('P-IY1', 'plea'),\n",
              " ('P-UW1', 'plew'),\n",
              " ('P-AW1', 'plough'),\n",
              " ('P-OW1', 'plough'),\n",
              " ('P-AW1', 'plow'),\n",
              " ('P-OY1', 'ploy'),\n",
              " ('P-UW1', 'plue'),\n",
              " ('P-AY1', 'ply'),\n",
              " ('P-CH', 'poach'),\n",
              " ('P-G', 'poag'),\n",
              " ('P-K', 'poch'),\n",
              " ('P-CH', 'poche'),\n",
              " ('P-K', 'pock'),\n",
              " ('P-D', 'pod'),\n",
              " ('P-JH', 'podge'),\n",
              " ('P-L', 'poehl'),\n",
              " ('P-ER0', 'poer'),\n",
              " ('P-Z', \"poe's\"),\n",
              " ('P-F', 'poff'),\n",
              " ('P-G', 'pog'),\n",
              " ('P-G', 'pogue'),\n",
              " ('P-L', 'pohl'),\n",
              " ('P-Z', 'poise'),\n",
              " ('P-K', 'poke'),\n",
              " ('P-L', 'pol'),\n",
              " ('P-L', 'pole'),\n",
              " ('P-K', 'polk'),\n",
              " ('P-L', 'poll'),\n",
              " ('P-M', 'pom'),\n",
              " ('P-N', 'pon'),\n",
              " ('P-NG', 'pong'),\n",
              " ('P-CH', 'pooch'),\n",
              " ('P-F', 'poof'),\n",
              " ('P-D', 'poohed'),\n",
              " ('P-L', 'pool'),\n",
              " ('P-L', 'poole'),\n",
              " ('P-N', 'poon'),\n",
              " ('P-P', 'poop'),\n",
              " ('P-R', 'poor'),\n",
              " ('P-R', 'poore'),\n",
              " ('P-P', 'pop'),\n",
              " ('P-P', 'pope'),\n",
              " ('P-P', 'popp'),\n",
              " ('P-P', 'poppe'),\n",
              " ('P-R', 'por'),\n",
              " ('P-R', 'pore'),\n",
              " ('P-R', 'porr'),\n",
              " ('P-S', 'pos'),\n",
              " ('P-SH', 'posch'),\n",
              " ('P-Z', 'pose'),\n",
              " ('P-SH', 'posh'),\n",
              " ('P-S', 'poss'),\n",
              " ('P-S', 'posts'),\n",
              " ('P-T', 'pot'),\n",
              " ('P-T', 'pote'),\n",
              " ('P-TH', 'poth'),\n",
              " ('P-T', 'pott'),\n",
              " ('P-CH', 'pouch'),\n",
              " ('P-L', 'poul'),\n",
              " ('P-R', 'pour'),\n",
              " ('P-T', 'pout'),\n",
              " ('P-ER0', 'power'),\n",
              " ('P-Z', 'pows'),\n",
              " ('P-ER0', 'poyer'),\n",
              " ('P-EY1', 'pray'),\n",
              " ('P-IY1', 'pre'),\n",
              " ('P-IY1', 'pree'),\n",
              " ('P-UW1', 'prew'),\n",
              " ('P-EY1', 'prey'),\n",
              " ('P-AY1', 'pri'),\n",
              " ('P-IY1', 'pri'),\n",
              " ('P-IY1', 'prix'),\n",
              " ('P-OW1', 'pro'),\n",
              " ('P-AW1', 'prough'),\n",
              " ('P-AW1', 'prow'),\n",
              " ('P-UW1', 'pru'),\n",
              " ('P-UW1', 'prue'),\n",
              " ('P-UW1', 'prugh'),\n",
              " ('P-AY1', 'pry'),\n",
              " ('P-Z', \"p's\"),\n",
              " ('P-Z', \"p.'s\"),\n",
              " ('P-Z', 'p.s'),\n",
              " ('P-UW1', 'pshew'),\n",
              " ('P-B', 'pub'),\n",
              " ('P-CH', 'puche'),\n",
              " ('P-K', 'puck'),\n",
              " ('P-T', 'puett'),\n",
              " ('P-F', 'puff'),\n",
              " ('P-G', 'pug'),\n",
              " ('P-UW1', 'pugh'),\n",
              " ('P-L', 'puhl'),\n",
              " ('P-G', 'puig'),\n",
              " ('P-L', 'pull'),\n",
              " ('P-N', 'pun'),\n",
              " ('P-NG', 'pung'),\n",
              " ('P-P', 'pup'),\n",
              " ('P-JH', 'purge'),\n",
              " ('P-K', 'purk'),\n",
              " ('P-Z', 'purrs'),\n",
              " ('P-S', 'purse'),\n",
              " ('P-T', 'purt'),\n",
              " ('P-S', 'pus'),\n",
              " ('P-SH', 'pusch'),\n",
              " ('P-SH', 'push'),\n",
              " ('P-S', 'puss'),\n",
              " ('P-S', 'puss'),\n",
              " ('P-T', 'put'),\n",
              " ('P-TH', 'puth'),\n",
              " ('P-CH', 'putsch'),\n",
              " ('P-T', 'putt'),\n",
              " ('P-K', 'pyke'),\n",
              " ('P-L', 'pyle'),\n",
              " ('P-M', 'pymm'),\n",
              " ('P-N', 'pyne'),\n",
              " ('P-ER0', 'pyre')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfd = nltk.ConditionalFreqDist(p3)\n",
        "cfd['P-P']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNheTj9-HiOy",
        "outputId": "844a9d73-e0da-44f9-b451-ce62d9168120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'paap': 1, 'paape': 1, 'pap': 1, 'pape': 1, 'papp': 1, 'paup': 1, 'peep': 1, 'pep': 1, 'pip': 1, 'pipe': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for template in cfd.conditions():\n",
        "  if len(cfd[template]) > 10:\n",
        "    words = cfd[template].keys()\n",
        "    wordlist = ' '.join(words)\n",
        "    print(template, wordlist[:70] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3hdlsE0Itye",
        "outputId": "ffbf7c9a-b185-499b-adee-a956a1e04647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P-P paap paape pap pape papp paup peep pep pip pipe pipp poop pop pope pop...\n",
            "P-R paar pair par pare parr pear peer pier poor poore por pore porr pour...\n",
            "P-K pac pack paek paik pak pake paque peak peake pech peck peek perc perk ...\n",
            "P-S pace pass pasts peace pearse pease perce pers perse pesce piece piss p...\n",
            "P-L pahl pail paille pal pale pall paul paule paull peal peale pearl pearl...\n",
            "P-N paign pain paine pan pane pawn payne peine pen penh penn pin pine pinn...\n",
            "P-Z pais paiz pao's pas pause paws pays paz peas pease pei's perz pez pies...\n",
            "P-T pait pat pate patt peart peat peet peete pert pet pete pett piet piett...\n",
            "P-CH patch pautsch peach perch petsch petsche piche piech pietsch pitch pit...\n",
            "P-UW1 peru peugh pew plew plue prew pru prue prugh pshew pugh...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prondict = nltk.corpus.cmudict.dict()\n",
        "prondict['fire']\n",
        "[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]\n",
        "prondict['blog'] = [['B', 'L', 'AA1', 'G']]\n",
        "prondict['blog']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLN-czCK-5nv",
        "outputId": "9111cf00-2ca9-41ab-8c09-af161a92ec55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['B', 'L', 'AA1', 'G']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prondict['natural'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmRffdr3J6Rw",
        "outputId": "1714e631-9ae6-425f-d65f-42aca83eb2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N', 'AE1', 'CH', 'ER0', 'AH0', 'L']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['natural', 'language', 'processing']\n",
        "[ph for w in text for ph in prondict[w][0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPN7RVjn_svO",
        "outputId": "e9533a70-348a-490f-a768-92d92a337a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N',\n",
              " 'AE1',\n",
              " 'CH',\n",
              " 'ER0',\n",
              " 'AH0',\n",
              " 'L',\n",
              " 'L',\n",
              " 'AE1',\n",
              " 'NG',\n",
              " 'G',\n",
              " 'W',\n",
              " 'AH0',\n",
              " 'JH',\n",
              " 'P',\n",
              " 'R',\n",
              " 'AA1',\n",
              " 'S',\n",
              " 'EH0',\n",
              " 'S',\n",
              " 'IH0',\n",
              " 'NG']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#200 common words in several languages.\n",
        "from nltk.corpus import swadesh"
      ],
      "metadata": {
        "id": "b3Pv2SaVKZDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import swadesh\n",
        "swadesh.fileids()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJS7sG07Km66",
        "outputId": "90bbb7ae-084b-4adc-dd5b-6653c3973d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be',\n",
              " 'bg',\n",
              " 'bs',\n",
              " 'ca',\n",
              " 'cs',\n",
              " 'cu',\n",
              " 'de',\n",
              " 'en',\n",
              " 'es',\n",
              " 'fr',\n",
              " 'hr',\n",
              " 'it',\n",
              " 'la',\n",
              " 'mk',\n",
              " 'nl',\n",
              " 'pl',\n",
              " 'pt',\n",
              " 'ro',\n",
              " 'ru',\n",
              " 'sk',\n",
              " 'sl',\n",
              " 'sr',\n",
              " 'sw',\n",
              " 'uk']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "swadesh.words('en')"
      ],
      "metadata": {
        "id": "g28mzywlKrfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fr2en = swadesh.entries(['fr', 'en'])\n",
        "fr2en\n"
      ],
      "metadata": {
        "id": "AKOMGeh0LPb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "translate = dict(fr2en)\n",
        "translate['eau']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "jPKwY37tLe1H",
        "outputId": "c5e07be3-0168-44d8-8966-ba5bea7e3bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-749f930b2231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtranslate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr2en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eau'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'eau'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']\n",
        "for i in [139, 140, 141, 142]:\n",
        "  print (swadesh.entries(languages)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbAx4mXLMUU2",
        "outputId": "82004ed3-4191-4046-c677-c4f7857d0c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('say', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere')\n",
            "('sing', 'singen', 'zingen', 'cantar', 'chanter', 'cantar', 'canere')\n",
            "('play', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar, brincar', 'ludere')\n",
            "('float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar, boiar', 'fluctuare')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WordNet\n",
        "#Senses and Synonyms\n",
        "from nltk.corpus import wordnet as wn\n",
        "wn.synsets('motorcar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZhUCR__Mvy3",
        "outputId": "e2f0c7b0-ed68-4afc-95f7-8bb0a375f59c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.synset('car.n.01').lemma_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRL8XnFlM89E",
        "outputId": "eb3f83a7-fb26-4d4d-ebb8-ec9cd3a443a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Synset.lemma_names of Synset('car.n.01')>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('car.n.01').definition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol53aDFDNF6d",
        "outputId": "aa4db17e-af64-471a-9f02-93c8a673c9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Synset.definition of Synset('car.n.01')>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('car.n.01').examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlzVdyqPNSwS",
        "outputId": "6d6cd223-6361-4fab-fce7-c003eccea115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Synset.examples of Synset('car.n.01')>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('car.n.01').lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziRv6gcrNapl",
        "outputId": "154996e3-cfb6-4f7a-9859-daec1c85ed5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Synset.lemmas of Synset('car.n.01')>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.lemma('car.n.01.automobile')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEg0UyscNdlW",
        "outputId": "c746965d-192c-467a-f0a0-fea9a03d7207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lemma('car.n.01.automobile')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.lemma('car.n.01.automobile').synset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O_MfvKQNge4",
        "outputId": "6b3ac0ac-251f-43dd-f832-f6d6f2d47dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Lemma.synset of Lemma('car.n.01.automobile')>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.lemma('car.n.01.automobile').name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKk7ODHONi3w",
        "outputId": "4abd687d-385d-42c0-afc8-d7a0a8779f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Lemma.name of Lemma('car.n.01.automobile')>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets('car')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_IlRchxNt7C",
        "outputId": "304edc19-b110-4963-dae7-6474de6ce5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01'),\n",
              " Synset('car.n.02'),\n",
              " Synset('car.n.03'),\n",
              " Synset('car.n.04'),\n",
              " Synset('cable_car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for synset in wn.synsets('car'):\n",
        "  print( synset.lemma_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9_p7NbQNvQs",
        "outputId": "f3c6e267-c265-4328-b473-3ef4ff522281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Synset.lemma_names of Synset('car.n.01')>\n",
            "<bound method Synset.lemma_names of Synset('car.n.02')>\n",
            "<bound method Synset.lemma_names of Synset('car.n.03')>\n",
            "<bound method Synset.lemma_names of Synset('car.n.04')>\n",
            "<bound method Synset.lemma_names of Synset('cable_car.n.01')>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.lemmas('car')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0tR800GN9sp",
        "outputId": "b8fd2a97-36e8-468c-8c63-46814bc166db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('car.n.01.car'),\n",
              " Lemma('car.n.02.car'),\n",
              " Lemma('car.n.03.car'),\n",
              " Lemma('car.n.04.car'),\n",
              " Lemma('cable_car.n.01.car')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "motorcar = wn.synset('car.n.01')\n",
        "motorcar.hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xIKwGwAOOTH",
        "outputId": "6f852428-a536-43e1-f9f4-4b977128601a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('ambulance.n.01'),\n",
              " Synset('beach_wagon.n.01'),\n",
              " Synset('bus.n.04'),\n",
              " Synset('cab.n.03'),\n",
              " Synset('compact.n.03'),\n",
              " Synset('convertible.n.01'),\n",
              " Synset('coupe.n.01'),\n",
              " Synset('cruiser.n.01'),\n",
              " Synset('electric.n.01'),\n",
              " Synset('gas_guzzler.n.01'),\n",
              " Synset('hardtop.n.01'),\n",
              " Synset('hatchback.n.01'),\n",
              " Synset('horseless_carriage.n.01'),\n",
              " Synset('hot_rod.n.01'),\n",
              " Synset('jeep.n.01'),\n",
              " Synset('limousine.n.01'),\n",
              " Synset('loaner.n.02'),\n",
              " Synset('minicar.n.01'),\n",
              " Synset('minivan.n.01'),\n",
              " Synset('model_t.n.01'),\n",
              " Synset('pace_car.n.01'),\n",
              " Synset('racer.n.02'),\n",
              " Synset('roadster.n.01'),\n",
              " Synset('sedan.n.01'),\n",
              " Synset('sport_utility.n.01'),\n",
              " Synset('sports_car.n.01'),\n",
              " Synset('stanley_steamer.n.01'),\n",
              " Synset('stock_car.n.01'),\n",
              " Synset('subcompact.n.01'),\n",
              " Synset('touring_car.n.01'),\n",
              " Synset('used-car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "motorcar.hypernym_paths()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wgM5JyLOWXA",
        "outputId": "69069267-9eac-4278-de56-0fdc0ebfb484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Synset('entity.n.01'),\n",
              "  Synset('physical_entity.n.01'),\n",
              "  Synset('object.n.01'),\n",
              "  Synset('whole.n.02'),\n",
              "  Synset('artifact.n.01'),\n",
              "  Synset('instrumentality.n.03'),\n",
              "  Synset('container.n.01'),\n",
              "  Synset('wheeled_vehicle.n.01'),\n",
              "  Synset('self-propelled_vehicle.n.01'),\n",
              "  Synset('motor_vehicle.n.01'),\n",
              "  Synset('car.n.01')],\n",
              " [Synset('entity.n.01'),\n",
              "  Synset('physical_entity.n.01'),\n",
              "  Synset('object.n.01'),\n",
              "  Synset('whole.n.02'),\n",
              "  Synset('artifact.n.01'),\n",
              "  Synset('instrumentality.n.03'),\n",
              "  Synset('conveyance.n.03'),\n",
              "  Synset('vehicle.n.01'),\n",
              "  Synset('wheeled_vehicle.n.01'),\n",
              "  Synset('self-propelled_vehicle.n.01'),\n",
              "  Synset('motor_vehicle.n.01'),\n",
              "  Synset('car.n.01')]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sementics Similarities"
      ],
      "metadata": {
        "id": "W9OEVUzJOmt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "right = wn.synset('right_whale.n.01')\n"
      ],
      "metadata": {
        "id": "DUlzfm7dYb2b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "novel = wn.synset('novel.n.01')"
      ],
      "metadata": {
        "id": "--IcBSTxafs7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "right.lowest_common_hypernyms(right)"
      ],
      "metadata": {
        "id": "hxffb-tCahCQ",
        "outputId": "b998b0e1-93d5-4cef-eab8-228dee1a7d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('right_whale.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "right.lowest_common_hypernyms(novel)"
      ],
      "metadata": {
        "id": "y4DYEMHtZ6XX",
        "outputId": "0c88716a-fa62-4009-f70c-6f8049f9d556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('entity.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wn.synset('baleen_whale.n.01').min_depth()\n"
      ],
      "metadata": {
        "id": "Jdsrxqk1YXX-",
        "outputId": "550dee66-7271-440a-bc75-fb4d4ac7cf3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset('entity.n.01').min_depth()\n",
        "\n"
      ],
      "metadata": {
        "id": "Nj9lDQO1acEu",
        "outputId": "5b34fd69-1734-434e-fc7c-a73bf474ab5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "right.path_similarity(novel)"
      ],
      "metadata": {
        "id": "N4S2a3ehaOJy",
        "outputId": "0da67b35-c3a4-45a4-8438-f5fe34eaf118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.043478260869565216"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l2sA6Qm1_Hu5"
      }
    }
  ]
}